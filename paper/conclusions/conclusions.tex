\chapter{Conclusions}\label{ch:conclusions}

In this thesis the feasibility of anomaly detection by using recurrent neural networks was investigated. Chapter~\ref{ch:evaluation} shows that it is possible to build a monitoring system that detects anomalies in real-time using recurrent neural networks, as well as it being possible to run the same system on a previously captured data set. As discussed in Chapter~\ref{ch:evaluation}, the network can handle training at 198 actions per second by using 16 GPUs and 221 actions per second by using 20 CPUs. Most network's administrators will be able to run this system in real-time as it requires minimal resources, especially when not training in real-time. Especially since adding more or faster GPUs/CPUs is an easy way to increase the capacity of the network. Knowing that the 26,301 users (including computer users) at Los Alamos National Laboratory that contributed to the data set generated 1,051,430,459 events in 58 days, this leads to 329 actions per second for 26,301 users. In this scenario the system would have to store the weights of 26,301 users. Each of the 14,586 trainable weights is being represented by a python float32. This data type is 4 bytes per float32, which means that the total size of the weights in memory is 4 * 14,586 = 58344 bytes or around 58 KB per user. In the Los Alamos data set the total size of all users' networks would be a very manageable 1.4 GB. Adding around 8 GPUs or 8 CPUs to the setup described in Chapter~\ref{ch:evaluation} should be enough to handle both training and testing in real-time for the Los Alamos computer network, making this a very feasible method of anomaly detection. From this it can be concluded that using a recurrent neural network for anomaly detection is technologically feasible at least in this scenario. 

%cSpell:words zoph
One problem is finding the optimal network architecture and training it. Very few parameters can be chosen with absolute certainty or with results backing them up as the best parameters. As the data set is unlabeled, no objective measure of how good the system is at finding actual cyber-security attacks exists, making it very hard to find the optimal network architecture. This also prevents the choosing of the best possible features. Even though this will be something that will (most likely) always remain an issue when it comes to unlabeled data sets, more research could be done into the best configurations for a given problem without involving labels. This area (meta learning) is an area that is very active at the moment. There is some progress being made when it comes to supervised learning such as in~\cite{zoph2016neural}, however, no such advancements have been made yet in the area of unsupervised learning. For example into the area of generating a good model and setup for a given problem or for selecting good features for training from given data.

Another problem is knowing whether the flagged actions are actually intrusion attempts. This is and always will be something that only domain experts are able to verify. While from Chapter~\ref{ch:results} it can be concluded that the actions that have been investigated (Table \ref{tab:predicted_vs_actual_top} and Table~\ref{tab:predicted_vs_actual_second}) do indeed look like anomalies, there is no certainty over whether the actions the network flagged as anomalous are all anomalies and whether all anomalous actions have in fact been detected. Because of this, the system can only be a tool for domain experts to reduce the number of cases that need to be closely investigated, not one that can completely replace them. However, the system is shown to pick out very anomalous behavior (and a possible multi-user cyber-security attack), showing that this approach is very effective on top of being technologically feasible.