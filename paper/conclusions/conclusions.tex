\chapter{Conclusions}\label{ch:conclusions}

In this paper we investigated the feasibility of anomaly detection by using recurrent neural networks. Chapter~\ref{ch:evaluation} shows that it is possible to run a real-time IDS using recurrent neural networks, as well as it being possible to run this system on a previously captured data set. Seeing as the network can handle 214 actions per second by using 16 GPUs, most networks will be able to run this IDS in real-time, especially since adding more or faster GPUs is an easy way to increase the capacity of the network. Knowing that the Los Alamos National Laboratory has 12,425 employees who that caused 1,648,275,307 events in 58 days, this leads to 329 actions per second for 12,425 users. This means that adding around 8 GPUs should be enough to handle both training and testing in real-time for the Los Alamos network, making this a very feasible method of anomaly detection. From this we can conclude that using a recurrent neural network for anomaly detection is technologically feasible at least in this scenario. 

However knowing whether the found anomalies are actually intrusion attempts is and always will be something that only domain experts are able to comment on. While from Chapter~\ref{ch:evaluation} we can conclude that the actions we investigated do indeed look like anomalies, there is no certainty over whether the found anomalies are all anomalies and whether all anomalous actions have in fact been detected. Because of this, the network can only be a tool for domain experts to reduce the amount of users that need to be closely investigated, not one that can completely replace them. 

%cSpell:words zoph
Another problem was the tuning of the neural network itself and its parameters. Very few parameters can be chosen with absolute certainty or with results backing them up as the best parameters. As the data set is unlabeled no measure of accuracy of the network can be given, preventing the optimizing and fine tuning of the network, which also prevents the choosing of the best possible features. Even though this will be something that will (most likely) always remain an issue when it comes to unlabeled data sets, more research could be done into the best configurations for a given problem without involving labels. This area (meta learning) is an area that is very active at the moment. Lots of big strides are being made when it comes to supervised learning such as in~\cite{zoph2016neural}, however no such advancements have been made yet in the area of unsupervised learning. For example into the area of generating a good model and setup for a given problem or for selecting good features for training from given data.