\chapter{Evaluation}\label{ch:evaluation}

Because there are three major stages to the evaluation (preprocessing, training/testing, translating) as explained in Chapter~\ref{ch:methods}, and these three stages having different bottlenecks, not all experiments were run on the same computers. Both the preprocessing and translating stages require the reading of the entire data set file, making it very RAM intensive. They also only require CPU work, shifting the bottleneck to the CPU after reading the file. Preprocessing also allows for CPU parallelization, making more CPUs a very good thing to have. Because of the high RAM requirement, the first and third stages are run on a computer with 1.5TB of ram and 16 Intel Xeon E5--2630v3 CPUs running at 2.40GHz with 32 threads. Because preprocessing reduces the total amount of data, using a computer with less RAM is now possible.  Because of this the second task was ran on a computer with 1TB of ram, 20 Intel Xeon E5--2650v3 CPUs running at 2.30GHz with 40 threads and 8 dual-gpu boards containing two NVIDIA Tesla K80 GPUs each with 11.5GB of memory. 

In order to get an idea how long running everything on 100\% of the data set would take, three different percentages have been used: 0.1\%, 1\% and 5\%. Doing preprocessing took 38 seconds for 0.1\% of the data while it took 40m16s for 5\% of the data, both using 10 CPUs. A very rough estimate puts the duration of preprocessing the entire data set at about 40 hours also using 10 CPUs. Doing the training/testing stage with 16 GPUs took 1h51m on 0.1\% of the data set, while 1\% took about 10 hours and 5\% of the data took 62h0m36s, giving a rough estimate of 2000 hours for 100\% of the data set. Using 20 CPUs instead takes 55h51m3s on 5\% of the data set, giving an estimate of 1800 hours for 100\% of the data set. This mean that on in our scenario running experiments on the CPU is faster. Results may however vary based on the amount of CPUs or GPUs the experiments are ran on, making one of these two the faster one. The anomaly translation part generally only takes roughly 2 and a half hours, not varying much between data set sizes as all users need to be iterated through regardless and no other heavy CPU work is being done. The biggest time sink for this part is loading the data set file itself at about 2h15m. Putting this all together, the entire process takes 65h10m52s when using GPUs and 59h1m19s when using CPUs both for 5\% of the data. The 5\% data set contains 50,276,292 actions, meaning the network can handle about 214 actions per second on GPUs and 237 actions per second on CPUs, making this very fit for real-time anomaly detection. The actual testing stage (without training) takes even shorter, generally only taking a few seconds per user for all their actions, which would make a network that doesn't continue learning after the initial training even more feasible to run.