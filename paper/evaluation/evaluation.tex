\chapter{Performance Evaluation}\label{ch:evaluation}

There are three major stages to the evaluation (preprocessing, training/testing, post-processing) as explained in Chapter~\ref{ch:methods}. The bottleneck in all stages is the CPU, because of this all tasks were ran on a computer with 1TB of ram, 20 Intel Xeon E5--2650v3 CPUs running at 2.30GHz with 40 threads and 8 dual-gpu boards containing two NVIDIA Tesla K80 GPUs each with 11.5GB of memory. The training/testing stage can be ran on the GPU as well, shifting the bottleneck to the GPU. Because of this, this stage is either very CPU- or GPU-intensive depending on which of the two is chosen.

In order to get an idea how long running everything on 100\% of the data set would take, two different percentages have been used: 0.1\% and 1\%. Keep in mind that a lot depends on the way things are programmed and code optimizations might significantly reduce the amount of time the process takes. The code used in this thesis was not optimized in that aspect. It was only used to support the hypothesis that real-time anomaly detection is feasible on the hardware available to big computer networks. Doing preprocessing took 2h45m14s for 0.1\% of the data while it took 4h57m for 1\% of the data, both using 10 CPUs. Scaling this up linearly would put the duration of preprocessing the entire data set at about 250 hours, also using 10 CPUs. Doing the training/testing stage with 16 GPUs took 43h14m53s on 0.1\% of the data set, while 1\% took 172h25m. Since 1\% of the dataset contains 126,322,330 feature vectors, and 100\% of the data set should contain 1,051,430,459 rows, doing training/testing for 100\% of the data set should take approximately 1435h5m30s using GPUs. Using 20 CPUs instead takes about 152h35m on 1\% of the data set. This means that on in our scenario running the system on the CPU is approximately 12\% faster. Results may vary based on the number and architecture of CPUs or GPUs the system is executed on, making either CPUs or GPUs the faster option. The post-processing stage generally only takes roughly 5h30m, not varying much between data set sizes as all users need to be iterated through regardless and no other heavy CPU work is being done. Adding all of these times together leads to the following results. The entire process takes 48h30m7s for 0.1\% of the data set, 179h52m for 1\% of the data set and approximately 1617h23m40s for 100\% of the data set. See table~\ref{tab:times_taken} for an overview. 1\% of the data set contains 126,322,330 feature vectors, meaning the system can handle training at about 198 rows per second on GPUs and 221 rows per second on CPUs, making this a very good fit for real-time training. The actual testing stage (without training) takes even shorter, generally taking about 1/100th of the time the training stage took for that user. This makes a system that learns and detects anomalies in real-time feasible. A system that does not continue learning after the initial training would taken up even less resources. This means a system that learns periodically instead of in real-time is even more technically feasible.

\begin{table}[htbp]
	\centering
	\caption{The time every stage takes, by data set size}\label{tab:times_taken}
	\begin{tabular}{llll}
										  & 0.1\%     & 1\%     & 100\%             \\ \cline{2-4} 
	\multicolumn{1}{l|}{Preprocessing}    & 2h45m14s  & 4h57m   & \(\sim\)250h        \\
	\multicolumn{1}{l|}{Training/Testing} & 43h14m53s & 172h25m & \(\sim\)1435h5m30s \\
	\multicolumn{1}{l|}{Post-processing}      & 5h20m     & 5h20m   & 5h20m             \\
	\multicolumn{1}{l|}{Total}            & 51h20m7s  & 182h42m & \(\sim\)1690h25m30s (~70 days)
	\end{tabular}
\end{table}