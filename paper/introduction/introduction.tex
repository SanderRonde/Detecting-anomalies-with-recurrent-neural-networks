\chapter{Introduction}\label{ch:introduction}
As the presence of computer networks in our day-to-day lives increases, the need for a method to detect attacks or abuse of these networks also increases. This leads to network administrators keeping track of everything going on in the network in an attempt to pick out any weird behavior. The problem however is that this data needs an expert's opinion, while also needing to be processed very quickly as there tends to be a lot of this data. This calls for a computer system handling this problem as humans simply can't keep up with the amount of data. A system that does this is also known as an Intrusion Detection System (IDS). This IDS needs to be both fast and accurate, while at the same time being able to adapt to any changes the attackers might make to avoid it. The system should preferably also be able to run in real-time, being able to pick out any weird behavior as it happens, instead of finding out weeks after the fact which can be a very important factor for confidential data. The upcoming field of artificial neural networks (ANN) seems like a perfect fit for this problem, as it combines both the speed of computers, and attempts to mimic the ability of our brains to learn very quickly, allowing it to make good choices.

%cSpell:words akent
In 2015,~\cite{akent-2015-enterprise-data} published a dataset of around 100GB representing 58 consecutive days of de-identified event data collected from the US based Los Alamos National Laboratory's internal network. This dataset consists of a number of different types of data. These types are authentication data, process data, network flow data, DNS data and red team data, where the authentication data is by far the biggest at 1,648,275,307 events. Here the red team data represents a set of simulated intrusions. The red team data is there to train the system on known intrusions (also known as misuse detection) or to validate any found anomalies, however there is so little red team data that it is not feasible to do this. Seeing as the rest of the data is non-labeled data, where we do not know whether it actually is or isn't an anomaly, the system needs to be trained to recognize users' behavior and any deviations from this behavior (also known as anomalies). Because the data consists of series of actions, sequences of events that are only anomalies when seen together (also known as collective anomalies) might also be in the dataset. Collective anomalies would go unnoticed when only reading the data one action at a time, however a recurrent neural network, which specializes in series of data, is able to find these collective anomalies, making it a perfect fit.

It is recommended to end the introduction with an overview of the thesis. This chapter contains the introduction; Chapter~\ref{ch:related_work} discusses related work; Chapter~\ref{ch:conclusions} concludes.

Also make a nice sentence with ``bachelor thesis'', LIACS and the names of the supervisors.
%TODO:
