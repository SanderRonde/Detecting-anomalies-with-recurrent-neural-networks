\chapter{Introduction}\label{ch:introduction}
As the presence of computer networks in our day-to-day lives increases, the need for a method to detect attacks or abuse of these networks also increases. This leads to network administrators keeping track of everything going on in the network, in an attempt to pick out any weird behavior. The data of what goes on in the network is stored in so-called log files. The problem, however, is that this data needs an expert's opinion, while also needing to be processed very quickly as there tends to be a lot of this data. This calls for a computer system handling this problem as humans simply can't keep up with the amount of data. A system that does this needs to be both fast and accurate, while at the same time being able to adapt to any changes the attackers might make to avoid it. The system should preferably also be able to run in real-time, being able to pick out any weird behavior as it happens, instead of finding out weeks after the fact which can be a very important factor for confidential data. The upcoming field of Deep Neural Networks (DNNs) seems like a perfect fit for this problem, as it combines both the speed of computers and attempts to mimic the ability of our brains to learn very quickly, allowing it to make good choices.

%cSpell:words akent
In 2015, a data set\footnote{The data itself can be found here~\cite{kent-2015-cyberdata1}} was published in~\cite{akent-2015-enterprise-data} of around 100GB representing 58 consecutive days of anonymized event data collected from the US based Los Alamos National Laboratory's internal network. This data set consists of a number of different types of data: authentication data, process data, network flow data, DNS data and red team data, where the authentication data is by far the biggest at 1,648,275,307 events. Here the red team data represents a set of simulated intrusions. The red team data is there to train the system on known intrusions (also known as misuse detection) or to validate any found anomalies, however, there is so little red team data that it is not feasible to do this. Because the rest of the data is non-labeled, where we do not know whether it actually is or isn't an anomaly, the system needs to be trained to recognize users' behavior and any deviations from this behavior (also known as anomalies). Because the data consists of series of actions, sequences of events that are only anomalies when seen together (also known as collective anomalies) might also be in the data set. Collective anomalies would go unnoticed when only reading the data one action at a time, however, a recurrent neural network, which specializes in series of data, is able to find these collective anomalies, making it a perfect fit.

The main goal of this paper is to apply an LSTM to the data set in an effort to find anomalies. This is done by transforming the data set into features that are then used to train a network to predict the behavior of a user. Then the network is asked to predict the next action after being given some input, after which the prediction is compared to the actual action and any actions that deviate too much are classified as anomalies.

This thesis is structured as follows: in Chapter~\ref{ch:related_work} related work is discussed; in Chapter~\ref{ch:rnn} the RNN architecture is explained; in Chapter~\ref{ch:methods} the used methods are described; in Chapter~\ref{ch:ch:evaluation} the time taken to run the experiments is discussed; in Chapter~\ref{ch:results} the findings are presented and Chapter~\ref{ch:conclusions} contains the conclusion.