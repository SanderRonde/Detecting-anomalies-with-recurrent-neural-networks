\chapter{Methods}\label{ch:methods}

In the data set that is being used in this thesis, there is a lot of data that is very essential, that requires some processing to extract (such as whether the user connected to a computer they haven't visited before). This is the reason for so-called features being created based on the data. These features are then sent to the network to train after being split into a training and test set. Since users tend to have different behavioral patterns in the data set and every other big network, the decision was made to train one network per user. The possibility of using one network for all users was explored, but this introduced some problems like the total amount of actions for all users not being the same and problems with performance. The network also gravitates towards learning the behavior of \enquote{average} users. It will try to find a middle ground in the behavior of different types of users (sysadmins vs users that rarely log in), never really learning a single user's behavior well enough to find slight deviations. It will then accept outliers such as sysadmins having a high error value as normal, which could be very dangerous if such an account gets compromised. After training on the training set, the network then runs on the test set. A list is then composed containing the differences between the expected and actual values. Any items in this list that deviate too much from the median are then labeled as anomalies.

%cSpell:words akent
\section{Data}
The data set from~\cite{akent-2015-enterprise-data} contains a number of different types of data, as described in Chapter~\ref{ch:introduction}. For this thesis we will only be using the authentication data since that is by far the largest data set with 1,648,275,307 events. The data set has a total of 17,684 computers and spans 58 consecutive days. There are 26,301 users in the data set. %TODO: amount of computer users
of these users are computer users. These are not tied to specific persons and as such learning their behavior will not be very useful. As such these are not included in the testing/training sets. In addition to these users there is a single \enquote{anonymous user} that is also excluded from the testing/training sets. A minimum amount of 150 actions per user has also been chosen in order for them to be included in the testing/training sets. This number was chosen because, since weights are updated after every batch and a batch size of in this case 32, the weights can only be updated a few times, leading to a bad approximation of the user's behavior. It could be argued that this value should be even higher, but it should be fairly easy to determine if a user's behavior was only flagged as anomalous because they have so few actions the network hasn't learned their behavior yet, making this a very easy to fix problem. The data was entirely anonymized in addition to the time frame at which it was captured not being disclosed. The authentication data format can be seen in table~\ref{tab:data}

\begin{table}[htbp]
	\centering
	\caption{The data set structure}\label{tab:data}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lllllllll}
			time & source user@domain & destination user@domain & source computer & destination computer & authentication type & logon type & authentication orientation & success/failure \\ \midrule
			1    & C625@DOM1          & U147@DOM1               & C625            & C625                 & Negotiate           & Batch      & LogOn                      & Success         \\
			1    & C625@DOM1          & SYSTEM@C653             & C653            & C653                 & Negotiate           & Service    & LogOn                      & Success         \\
			1    & C625@DOM1          & SYSTEM@C653             & C660            & C660                 & Negotiate           & Service    & LogOn                      & Success        
		\end{tabular}
	}
\end{table}

%TODO: howmany users used at the end
Due to the size of the data set and the limited amount of time, the decision was made to use only 5\% of the users for plots/results in this thesis. This means the first 5\% of valid users, meaning only non-anonymous human users, as explained in the previous section. Users are chosen regardless of the amount of actions they have and are simply sorted alphabetically, after which the first 5\% are taken.

\begin{table}[htbp]
	\centering
	\caption{The features}\label{tab:features}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lll}
			Index & Feature                & Description                                                                \\ \midrule
			0     & domains delta          & 1 if a previously unvisited domain was accessed, 0 otherwise               \\
			1     & dest users delta       & 1 if a previously unvisited destination user was accessed, 0 otherwise     \\
			2     & src computers delta    & 1 if a previously unvisited source computer was accessed, 0 otherwise      \\
			3     & dest computers delta   & 1 if a previously unvisited destination computer was accessed, 0 otherwise \\
			4     & time since last access & The time (in seconds) since the last time any network activity occurred    \\
			5     & auth type              & What type of authentication type was used (one of enum)                    \\
			6     & logon type             & What type of logon type was used (one of enum)                             \\
			7     & auth orientation       & What type of authentication orientation was used (one of enum)             \\
			8    & success or failure     & 1 if the login succeeded, 0 if it didn't                                  
		\end{tabular}
	}
\end{table}

\begin{table}[htbp]
	\centering
	\caption{One of encoding}\label{tab:one_of_encoding}
	\begin{tabular}{llll}
	Value & A & B & C \\ \midrule
	1     & 1 & 0 & 0 \\
	2     & 0 & 1 & 0 \\
	3     & 0 & 0 & 1
	\end{tabular}
\end{table}

\section{Features}
As the raw data has some unneeded values such as the domains and that need to be transformed such as the time since the last access, features are constructed from the original rows. Since increasing the number of features has a big performance impact, keeping the number of features low, while still making sure the most important features are extracted is very important. The features are taken from the actions of one user over the whole data set. For the features see table~\ref{tab:features}. All enum values (indexes 5,6,7) have been encoded using 1-of-encoding (see table~\ref{tab:one_of_encoding} for an example), ensuring the values remain nominal. This gives each possible value a single spot in a vector, setting it to 1 only if that value is true, and setting the rest to 0. This allows the network to make individual predictions for every possible enum value instead of possibly assigning them a real value. This brings the total length of the feature vector up to 33 (11, 9 and 6 possible values for the enums respectively).

The features have been chosen to fill the values that the network will probably be using. For example, the network is unlikely to keep track of a list of every computer the user logged into, but would probably be interested in knowing whether the user logged in to a new computer. Additionally the network is unlikely to subtract the previous action's time stamp from the current time stamp, but will probably be interested in knowing the time since the last action to see whether the user is doing lots of operations at once, is doing actions at a normal human speed, or if they're barely doing anything.

\section{Preprocessing}
In order to have both a training and test set for every user, the data is split up. 70\% is used for training and 30\% is used for the test set. This is done separately for every user, making sure that each user has the same 70--30 split. The network expects only real-valued numbers from the range [0,1], however, because the features contain integer values larger than 1, these values have to be normalized to fit into this range. This is done by taking the maximum value for every column and dividing every value in that column by that maximum, linearly scaling every value down to the range [0,1]. This is done by applying the formula below to every column per user, where \(x\) is the input column and \(x'\) is the output column:

$$ x' = x / \max (x) $$

This operation is performed for the training and test set before they are split up, ensuring that the scaling factor is the same for both sets. The data is kept in chronological order as it was read originally, ensuring that the input data closely resembles real input data and making maximum use of the LSTM's ability to make sense of sequences.

Keep in mind that in a real-time scenario, scaling can not be done by using the same factor for both the training and test set as the eventual maximum value is unknown, leading to values that fall above the [0,1] range. This can be solved by taking the maximum possible or reasonable value as a scaling factor for both test sets (for example no user will ever access more computers than are available on the network and no user will have more seconds between their last action than there are in a human lifetime). One other method of solving this problem is by applying the following function to all (unscaled) feature values:

$$ x' = \dfrac{1}{1+x} $$

Instead of continuously increasing, \(x'\) shrinks here, fixing the problem of features exceeding the range [0,1]. This also takes care of scaling the feature down to the range [0,1]. This would be a good solution to the problem, however, as we are not using real-time data in the experiments this method is not needed.

\section{Experimental setup}
This network consists of 3 layers, with the first two being stateful LSTMs (stateful meaning the state is preserved across batches), and the third layer being a dense layer which transforms the data to the correct vector length. All 3 layers use an internal representation vector (and layer output) size of \(feature\_size \), which is \enquote{between} the network's input and output sizes, both being \(feature\_size \) as well (as suggested in~\cite{heaton2008introduction}). This leads to a total amount of 4,888 trainable weights. The output vector then consists of real values representing each feature separately. The network uses a batch size of 32. Increasing the batch size tends to cause the network to converge slower, which can cause problems when dealing with users with few actions. On the other hand reducing the batch size quickly slows down the network by a lot. The network is trained first on the supplied training data, always trying to optimize for the lowest loss value using the mean squared error function (mse). This function measures the average of the squares of the differences between actual and predicted values, giving an approximation of the deviation from the expected value. The mean squared error function is calculated by using the following formula with \(n\) being the length of the input vector, \(x\) being the predicted vector and \(y\) being the actual vector:

$$ mse = (\sum\limits_{i=0}^{n - 1} {(x_i - y_i)}^2) / n $$

The \(mse\) function is then applied to the predicted feature vector and the actual feature vector. This is done by using the \enquote{adam} optimizer. The learning rate of this optimizer was set to 0.001. Training is repeated for 25 epochs. This value was chosen because increasing this value would introduce overfitting and reducing it would result in higher loss values overall. As another measure to prevent overfitting, a dropout factor of 0.5, and a recurrent dropout factor of 0.2 is used for both LSTM layers. A dropout factor, which randomly drops certain input vectors, and a recurrent dropout factor that randomly drops out vectors between states, were shown to prevent overfitting in~\cite{srivastava2014dropout}. Note that these parameters are not perfect and they are all chosen because they are either standard values in many projects (batch size and epochs) or because they are recommended (dropout). Because of the use of unsupervised learning, no measure of how good the network actually is at detecting attacks exists. Because of this no objective measure of how good the network exists, and the network's parameters can not be optimized by using this measure. If a parameter has not been mentioned here, the value of that parameter is keras' default value for that parameter.

%cSpell:words srivastava
\section{Training}
After preprocessing, the training data is then used as input for the networks. For performance reasons, a single network is created, which is then used as the template for every other network. Instead of creating a new network for every user, new weights are created that are then applied to the base network. The network is trained by inputting the a feature vector in the training set sequence and having it predict the next feature vector, after which the \(mse\) over these two vectors is calculated. This is done for every feature vector in the training set in batches of 32. After every batch the weights are then updated based on the error value. This is repeated for all batches in the training set.

\section{Testing}\label{sec:methods:testing}
After training, the network is applied to the test set. There is a limitation requiring the use of the same batch size for both training and testing, which would downplay the significance of single anomalies (as in a set of \(batch\_size\) losses, one anomaly is not that significant), a method needs to be devised to test on a batch size of 1 instead. This is solved by creating another network, identical in structure, and transferring the weights and states when tests occur. This has the same effect as changing the original network's batch size to 1 (for this application), but without all the performance losses.

The losses from all of the test data are then collected, after which the interquartile range (IQR) is calculated over the training set's error values. The IQR function attempts to find statistical outliers based on the median values of a distribution. This is done by calculating the medians of both the upper and lower half of a distribution, which are then called Q1 and Q3 respectively. The IQR is then equal to \(Q3 - Q1\). Any values that lay outside of the ranges of the following functions, where \(x\) is the input value, are then called outliers.

$$ x < Q1 - 1.5 IQR $$

$$ x > Q3 + 1.5 IQR $$

In practice the first form of outlier will almost never be found, as that would mean an action so perfectly fits the user, it is an anomaly, which is very unlikely to point to actual anomalous behavior. Because the training set's error values should represent a regular sequence of actions by the user, any error values in the test set that fall outside of the range calculated with the above functions are classified as anomalies.

After finding anomalies, they have to be translated into actual rows. This issue occurs because when inputting solely features, the original input is discarded. As network administrators will want to see the name of the user that is behind a found anomaly and may want to investigate the actions themselves, these anomalies are translated back into source actions. This is done by storing the index of an anomaly as well as the user associated with the anomaly. The indexes have a 1--to--1 correspondence to the source actions, allowing for easy translation. This step can be skipped if, for example, no anomalies were found or only known anomalies were found.

%cSpell:words chollet tensorflow whitepaper
\section{Code}
All the code was written in Python, using the~\cite{chollet2015keras} Keras deep learning wrapper's LSTM as the neural network, using all default settings except for those mentioned. TensorFlow~\cite{tensorflow2015-whitepaper} was used as the underlying library for Keras. Due to both the preprocessing/feature generation and the training/testing stages being very slow (as will be explained later in the evaluation section), especially when using big datasets, both of these operations can be fully parallelized. The first stage (preprocessing/feature generation) is a very CPU-dependent task, this task can be split over any amount of CPU's, handling a single user per CPU until all users have been processed. The second stage (training/testing) can be either run on the CPU (s) or GPU (s). Depending on the hardware of the computer the experiments are executed on, one of these will be faster, as will be discussed in Chapter~\ref{ch:evaluation}. When using the CPU Keras itself will use all CPUs available to it, however when using the GPU a problem arises. Because Keras only uses a single GPU per neural network a problem arises where only a single GPU can be used per model, which in our case is the template model. This can be solved by splitting the work into multiple independent processes in order to parallelize. This is done by having one root process splitting the to-do job between \(n\) processes. These \(n\) processes all produce partial outputs (both anomalies and plots), that have to then be stitched together by the host process. The host process then produces the final output.