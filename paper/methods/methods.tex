\chapter{Methods}\label{ch:methods}

In the original data set that is being used in this thesis, there is a lot of hidden information is essential, for example whether the user connected to a computer he/she hasn't visited before. This is the reason for so-called features being created based on the data. These features are then sent to the network to train on, after being split into a training set and a test set. Since individual users tend to have different behavioral patterns, the decision was made to train one network per user. Another option was explored, as explained in Chapter~\ref{ch:experiments}. After training on the training set, the network is then applied to the test set. A list is then composed for the training set and the test set, containing the differences between the expected and actual vectors. Any elements in the list of the test set deviating too much from the median of the training set's list (which should represent normal behavior), are then labeled as anomalies.

%cSpell:words akent
\section{Data}
The data set from~\cite{akent-2015-enterprise-data} contains a number of different types of data, as described in Chapter~\ref{ch:introduction}. In this thesis, only the authentication data will be used since that is by far the largest data set with 1,051,430,459 events. The data set covers a total of 17,684 computers and spans 58 consecutive days. There are 26,301 total users in the data set, 13,875 of which are computer users. These are not tied to specific persons and as such learning their behavior will not be very useful. As such these are not included in the testing/training sets. In addition to these users there is a single \enquote{anonymous user} that is also excluded from the testing/training sets. This leaves a total of 12,425 human users. A minimum number of 150 actions per user has also been chosen in order for them to be included in the testing/training sets. The number of users that do not meet this criterion is not known as the system has not been run on 100\% of the data set. The number 150 was chosen because, since weights are updated after every batch, with a batch size of in this case 32, the weights can only be updated a few times, leading to a bad approximation of the user's behavior. It could be argued that the minimum number of actions should be even higher. However, it should be fairly easy to determine if a user's behavior was only flagged as anomalous because they have so few actions that the network hasn't learned their behavior yet, making this a small problem. The data was entirely anonymized. Also the true dates over which data were collected has not been disclosed. The authentication data format can be seen in Table~\ref{tab:data}

\begin{table}[htbp]
	\centering
	\caption{The data set structure}\label{tab:data}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lllllllll}
			time & source user@domain & destination user@domain & source computer & destination computer & authentication type & logon type & authentication orientation & success/failure \\ \midrule
			1    & C625@DOM1          & U147@DOM1               & C625            & C625                 & Negotiate           & Batch      & LogOn                      & Success         \\
			1    & C625@DOM1          & SYSTEM@C653             & C653            & C653                 & Negotiate           & Service    & LogOn                      & Success         \\
			1    & C625@DOM1          & SYSTEM@C653             & C660            & C660                 & Negotiate           & Service    & LogOn                      & Success        
		\end{tabular}
	}
\end{table}

Only human users having more than 150 records are used in this thesis. Since anonymous users and computer users do not represent a single person, and the persons that use a given computer may change at any time, attempting to model the behavior of these users is not effective.

\section{Features}
Some variables like the computer's name can not be used as input for a neural network, instead they are used to define features. Since increasing the number of features has a big performance impact, keeping the number of features low, while still making sure the most important data is represented by them is essential. The features are created on a per-user basis, iterating through that user's events and generating features based on them. For an overview of the features see Table~\ref{tab:features}. All categorical values (indices 5,6,7) have been encoded using 1-of-encoding (see Table~\ref{tab:one_of_encoding} for an example), ensuring the values remain categorical. This gives each possible categorical value a single spot in a vector, setting it to 1 only if that value is true, and setting the rest to 0. This allows the network to make individual predictions for every possible categorical value, instead of having to output a single prediction for all values through one real-numbered output. This brings the total length of the feature vector up to 33 (11, 9 and 6 possible values for the categorical values respectively).


\begin{table}[htbp]
	\centering
	\caption{The features}\label{tab:features}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lll}
			Index & Feature                		& Description                                                                \\ \midrule
			0     & domains delta          		& 1 if a previously unvisited domain was accessed, 0 otherwise               \\
			1     & dest users delta       		& 1 if a previously unvisited destination user was accessed, 0 otherwise     \\
			2     & src computers delta    		& 1 if a previously unvisited source computer was accessed, 0 otherwise      \\
			3     & dest computers delta   		& 1 if a previously unvisited destination computer was accessed, 0 otherwise \\
			4     & time since last access 		& The time (in seconds) since the last time any network activity occurred    \\
			5     & auth type              		& What type of authentication type was used (categorical data)                   \\
			6     & logon type            		& What type of logon type was used (categorical data)                            \\
			7     & auth orientation       		& What type of authentication orientation was used (categorical data)            \\
			8     & percentage failed logins 	& The percentage of logins that were unsuccessful							 \\
			9     & success or failure     		& 1 if the login succeeded, 0 if it didn't                                  
		\end{tabular}
	}
\end{table}

%cSpell:words multicolumn
\begin{table}[htbpH]
\centering
\caption{1-of-encoding}\label{tab:one_of_encoding}
\begin{tabular}{l|lll}
  & \(x_1\) & \(x_2\) & \(x_3\) \\ \hline
A & 1  & 0  & 0  \\
B & 0  & 1  & 0  \\
C & 0  & 0  & 1 
\end{tabular}
\caption{The translation of three values (A, B and C) into a vector representing each one.}
\end{table}


The features have been chosen in such a way to contain relevant information that ca be used for modeling user behavior. For example, the neural network is unlikely to keep track of every computer the user logged into, instead, the data might contain information about whether the user logged in to a computer they haven't previously logged in to. Additionally, the neural network is unlikely to subtract the previous action's time stamp from the current action's time stamp, but will probably be interested in knowing the time since the last action. This can be helpful for determining whether the user is doing lots of operations at once, doing them at a normal human speed, or if they're barely doing anything at all.

\section{Preprocessing}
In order to have both a training and test set for every user, the data is chronologically split up. 70\% is used for training and 30\% is used for the test set. This is done separately for every user, making sure that each user has the same 70--30 split. As every feature except for time since last access falls in the range [0,1], this feature also needs to be fit into that range. This is done by taking the maximum value for the time since last access column and dividing every value in that column by the maximum value in the column, linearly scaling every value down to the range [0,1].

The data is kept in chronological order, as it was read from the data set file, ensuring that the input data closely resembles the input data as it would appear in a real network, and making use of the LSTM's ability to make sense of sequences.

Keep in mind that in a real-time scenario, scaling can not be done by using the same factor for both the training and test set, as the eventual maximum value is unknown, leading to values that fall above the [0,1] range. This can be solved by taking the maximum possible or reasonable value as a scaling factor for both test sets. For example no user will ever access more computers than are available on the network and no human user will have more seconds between their last action than there are in a human lifetime. Another method of solving this problem is to apply the following function to all (unscaled) feature values:

\begin{equation} \label{eq:normalize_2}
x' = \dfrac{1}{1+x}
\end{equation}

Instead of continuously increasing, \(x'\) shrinks here, fixing the problem of features exceeding the range [0,1]. This also takes care of scaling the feature down to the range [0,1]. As such this is a very good solution to this problem. However, because no real-time training/testing occurs in this thesis, the problem does not have to be dealt with.

% FIRST LAYER

% KERNEL = W
% (forget = units
% input = units
% hidden state = units
% output = units)
% total = units * 4 * input_dim

% RECURRENT_KERNEL = U = hidden state =
% (forget = units
% input = units
% hidden state = units
% output = units) * units
% total = units * units * 4

% bias = (forget = units
% input = units
% hidden state = units
% output = units)
% total = units * 4

% total total = (units * 4 * input_dim) + (units * units * 4) + (units * 4) = (input_dim + 2) * units * 4


% SECOND LAYER

% input_dim is 33 instead, input_shape = (32, 33, 33)


% THIRD LAYER

% (units * input_dim) = kernel
% + units = bias
% total = (units * input_dim) + units

%cSpell:words glorot hahnloser
\section{Experimental setup}
The system consists of 3 layers, with the first two being stateful LSTMs (stateful meaning the state is preserved across batches), and the third layer being a dense layer using the ReLU activation function which was introduced in~\cite{hahnloser2000digital}. The 3 layers contain \(feature\_size\) (33) cells each, which is \enquote{between} the network's input and output sizes, both being \(feature\_size\) as well (as suggested in~\cite{heaton2008introduction}). This means that each layer's output vector is also of size \(feature\_size\). The dense layer transforms the data to the desired output dimensions of (\(feature\_size\), 1) where. All three layers have biases enabled and use the default keras initializer \enquote{glorot\_uniform}, introduced in~\cite{glorot2010understanding}, which draws its values from a uniform distribution in the range of [-limit, limit]. Limit is calculated as follows, with fan\_in being the weight's input dimension and fan\_out being the weight's output dimension (both always equal to \(feature\_size\) except for the first layer's kernel weights, where fan\_in is 1):

\begin{equation}
limit = sqrt(6 / (fan\_in + fan\_out))
\end{equation}

The first LSTM layer also returns its sequences instead of only last output, allowing the second LSTM to be stateful. The total number of weights for the LSTM layers can be calculated by using formula~\ref{eq:lstm_weights}. The number of units for both LSTM layers is \(feature\_size\) and the input shape for the first layer is (33, 1), leading to a total of 4620 trainable weights for the first layer. Due to the first layer returning its sequences, the second layer's input shape is (33, 33) leading to a total of 8844 for the second layer. The third layer is a dense layer, whose weights consist of a kernel of size \((input\_dim * units)\) and a set of weights of size \(units\), making the number of weights for the dense layer 1122. This leads to a total number of trainable weights. The output vector of shape (\(feature\_size\), 1) then holds the predicted features, where every value in this vector represents one feature. The network uses a batch size of 32. Increasing the batch size tends to cause the network to converge slower, which can cause problems when dealing with users with few actions. On the other hand reducing the batch size quickly slows down the network significantly. The network is first trained on the supplied training data, always trying to optimize for the lowest loss value, calculated by the mean squared error function (mse). This function measures the average of the squares of the differences between actual and predicted values, giving an approximation of the deviation from the expected value. The mean squared error is calculated by using the following formula with \(n\) being the length of the vector that was used as its input, \(x\) being the predicted vector and \(y\) being the actual vector:

\begin{equation} \label{eq:mse}
mse = (\sum\limits_{i=0}^{n - 1} {(x_i - y_i)}^2) / n
\end{equation}

The weights are adjusted during the training process to minimize the \(mse\) over the training set, one batch (of size 32) at a time. This is done by using the \enquote{adam} optimizer. The learning rate of this optimizer was set to 0.001. Training is repeated for 25 epochs. Because too many iterations of the optimizer might lead to overfitting, the training process was stopped at 25 epochs. Too little iterations, however, results in higher error values. This is also the number of epochs that is commonly used, making it a good choice when no other number of epochs was proven to work better. In Chapter~\ref{ch:experiments} some experiments regarding changing the epoch size are done. As another measure to prevent overfitting, a dropout factor of 0.5, and a recurrent dropout factor of 0.2 is used for both LSTM layers. A dropout factor, which randomly drops certain nodes in the network, and a recurrent dropout factor that randomly drops out vectors between states, were shown to prevent overfitting in~\cite{srivastava2014dropout}. Note that these parameters are not perfect and they are all chosen because they are either standard values in many projects (batch size and epochs) or because they are recommended (dropout). If a parameter has not been mentioned here, the value of that parameter is Keras' default value for that parameter. The system's goal is to identify outliers, which are then labeled as possible cyber-security attacks. Unfortunately, because the data set is unlabeled, no objective measure of how good the system is at finding actual cyber-security attacks exists, disallowing the optimization of the system's parameters based on this performance measure.

Because training LSTM networks requires many experiments with different setups and architectures, the decisions was made to use only 0.1\% of the users in the data set for Chapter~\ref{ch:experiments}. However, in order to get a clear idea of the system's performance and to measure the effectiveness of the system, 1\% of the users in the data set were used in Chapters~\ref{ch:evaluation} and~\ref{ch:results}. The users are sorted alphabetically before taking the first x\% of human users.

%cSpell:words srivastava
\section{Training}
After preprocessing, the training data is used as input for the networks. For performance reasons, a single network is created, which is then used as a template for every user. New weights are created for every user, which are then applied to the base network. The network is trained by inputting a feature vector in the training set sequence and having it predict the next feature vector (see Table~\ref{tab:training_set_shift}), after which the \(mse\) over these two vectors is calculated. This is done for every feature vector in the training set in batches of 32. After every batch the weights are updated based on the errors observed over that batch. This is repeated for all batches in the training set.

\begin{table}[H]
	\centering
	\caption{The network's input vector and the vector it's supposed to predict}\label{tab:training_set_shift}
	\begin{tabular}{ll}
		Input & Target Prediction \\ \hline
		x_0    & x_1                \\
		x_1    & x_2                \\
		x_2    & x_3                \\
		\dots   & \dots               \\
		x_{n-2}  & x_{n-1}              \\
		x_{n-1}  & x_n               
	\end{tabular}
\end{table}

\section{Testing}\label{sec:methods:testing}
After training, the network is applied to the test set. In Keras the batch size of the training set and the batch size of the test set have to be the same, this downplays the significance of single anomalies. For example, in a set of \(batch\_size\) errors, one big anomaly is not as significant as 10 small anomalies. As such a method needs to be devised to test using batch size of 1 instead. This is done by creating another network, identical in structure but having a batch size of 1, and transferring the weights and states when tests occur. This has the same effect as changing the original network's batch size to 1 (for this application), but without all the performance losses.

First of all, the interquartile range (IQR) is calculated over the training set's errors in order to get a baseline for the test set. The IQR function attempts to find statistical outliers based on the median values of a distribution. This is done by calculating the medians of both the upper and lower half of a distribution, which are then called Q1 and Q3 respectively. The IQR is then equal to \(Q3 - Q1\). Any values that lay outside of the ranges of the following functions, where \(x\) is the input value, are then called outliers.

\begin{subequations}
\begin{align}
	x < Q1 - 1.5 IQR \label{eq:iqr_min} \\
	x > Q3 + 1.5 IQR \label{eq:iqr_max}
\end{align}
\end{subequations}

The error represents the difference between the predicted vector and the actual vector. Because of this, the higher the error, the bigger this difference. This also means that the first form of outlier (\ref{eq:iqr_min}) is not interesting, as the model simply predicted the behavior very well, not pointing to anomalous behavior. Because the training set's errors should represent a regular sequence of actions by the user, any errors calculated over the test set that fall outside of the range calculated with the above functions are classified as anomalies.

After finding anomalies, the corresponding fragments of the original data should be investigated to find out if they represent cyber-security events. This issue occurs because when inputting solely features, the events they are based on are discarded. As network administrators will want to see the name of the user that is behind a found anomaly and may want to have the events investigated by a cyber-security expert, these anomalies are translated back into source events. This is done by storing the index of an anomaly as well as the user associated with the anomaly. The indices have a 1--to--1 correspondence to the source events, allowing for easy translation. This step can be skipped if, for example, no anomalies were found or only previously known anomalies were found.

%cSpell:words chollet tensorflow whitepaper
\section{Code}
All the code was written in Python, using the~\cite{chollet2015keras} Keras deep learning wrapper's LSTM as the neural network. Default settings were used if not mentioned otherwise. TensorFlow~\cite{tensorflow2015-whitepaper} was used as the underlying library for Keras. Due to both the preprocessing/feature generation and the training/testing stages being very slow (as will be explained later in the evaluation section), especially when using big datasets, both of these operations have been parallelized. The first stage (preprocessing/feature generation) is a very CPU-dependent task, this work can be split over any number of CPU's, handling a single user per CPU at a time until all users have been processed. The second stage (training/testing) can be either run on the CPU (s) or GPU (s). Depending on the hardware of the computer the experiments are executed on, one of these will be faster, as will be discussed in Chapter~\ref{ch:evaluation}. When using the CPU, TensorFlow itself will use all CPUs available to it, however when using the GPU, a problem arises. Because TensorFlow only requires a single GPU per neural network per process, and increasing the amount of GPUs per network does not speed up training or testing, there is no point in using multiple GPUs for the same network. Because of this, only a single GPU will be used per model in this thesis, which in this case is the template model. In order to make use of the other GPUs and to speed up computation, a method of splitting the work over all GPUs must be found. This can be done by splitting the work into multiple independent processes in order to parallelize the operations. This is done by having one root process splitting the to-do jobs between \(n\) processes. The total number of users is split over the sub-processes. These \(n\) processes all produce partial outputs (both anomalies and plots), that have to then be stitched together by the host process. The host process then produces the final output.