\chapter{Methods}\label{ch:methods}

In a data set as big as the one that is being used, there is a lot of information that is very essential but not included by default in a row (such as whether the user connected to an unvisited computer), while there is also some data that does not tell a lot when unprocessed (such as text data that the network shouldn't handle as a vector). This is the reason for the data being turned into so-called features. These features are then sent to the network to learn after being split into a training and test set. 
Because users behave very differently in the data set (and every big network), the decision was made to train one network per user. The possibility of using one network for all users was explored, but this introduced some problems like the amount of actions for all users not being the same, the network moving to recognize the median values, and simple performance reasons. After training on the training set, the network then runs on the test set. A list is then composed containing the differences between the expected and actual values. Any items in this list that deviate too much from the median are then labeled as anomalies.

%cSpell:words akent
\section{Data}
The data set from~\cite{akent-2015-enterprise-data} contains a number of different types of data, as described in the introduction. For this thesis we will only be using the authentication data as that is by far the largest data set at 1,648,275,307 events. The data set has a total of 12,425 users over 17,684 computers and spans 58 consecutive days. The data was entirely anonymized in addition to the time frame at which it was captured not being disclosed. The data format can be seen in table~\ref{tab:data}

\begin{table}[htbp]
	\centering
	\caption{The data set structure}\label{tab:data}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lllllllll}
			time & source user@domain & destination user@domain & source computer & destination computer & authentication type & logon type & authentication orientation & success/failure \\ \midrule
			1    & C625@DOM1          & U147@DOM1               & C625            & C625                 & Negotiate           & Batch      & LogOn                      & Success         \\
			1    & C625@DOM1          & SYSTEM@C653             & C653            & C653                 & Negotiate           & Service    & LogOn                      & Success         \\
			1    & C625@DOM1          & SYSTEM@C653             & C660            & C660                 & Negotiate           & Service    & LogOn                      & Success        
		\end{tabular}
	}
\end{table}

\begin{table}[htbp]
	\centering
	\caption{The features}\label{tab:features}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{lll}
			Index & Feature                & Description                                                                \\ \midrule
			0     & domains delta          & 1 if a previously unvisited domain was accessed, 0 otherwise               \\
			1     & dest users delta       & 1 if a previously unvisited destination user was accessed, 0 otherwise     \\
			2     & src computers delta    & 1 if a previously unvisited source computer was accessed, 0 otherwise      \\
			3     & dest computers delta   & 1 if a previously unvisited destination computer was accessed, 0 otherwise \\
			4     & time since last access & The time (in seconds) since the last time any network activity occurred    \\
			5     & auth type              & What type of authentication type was used (one of enum)                    \\
			6     & logon type             & What type of logon type was used (one of enum)                             \\
			7     & auth orientation       & What type of authentication orientation was used (one of enum)             \\
			8    & success or failure     & 1 if the login succeeded, 0 if it didn't                                  
		\end{tabular}
	}
\end{table}

\begin{table}[htbp]
	\centering
	\caption{One of encoding}\label{tab:one_of_encoding}
	\begin{tabular}{llll}
	Value & A & B & C \\ \midrule
	1     & 1 & 0 & 0 \\
	2     & 0 & 1 & 0 \\
	3     & 0 & 0 & 1
	\end{tabular}
\end{table}

\section{Features}
Because the raw data has some unneeded values, some that need to be transformed, and some that need to be added, features are made from the original rows. Because increasing the amount of features has a big performance impact, keeping the amount of features low, while still making sure the most important features are extracted is very important. The features are taken from the actions of one user over the whole data set. For the features see table~\ref{tab:features}. All enum values (indexes 5,6,7) have been encoded using 1-of-encoding (see table~\ref{tab:one_of_encoding} for an example), ensuring the values remain nominal. This gives each possible value a single spot in a vector, setting it to 1 only if that value is true, and setting the rest to 0. This allows the network to make individual predictions for every possible enum value instead of possibly assigning them a real value. This brings the total number of the feature vector up to 33 (11, 9 and 6 possible values for the enums respectively).

The features have been chosen to fill the values that the network will probably be using. For example the network is unlikely to keep track of a list of every computer the user logged in to, but would probably be interested in knowing whether the user logged in to a new computer. Additionally the network is unlikely to subtract the previous action's time stamp from the current time stamp, but will probably be interested in knowing the time since the last action to see whether the user is doing lots of operations at once, is doing actions at a normal human speed, or if they're barely doing anything.

\section{Preprocessing}
Not all users are can be used for training/testing. This is because some users have so little actions that no reasonable predictions can be made for them. The minimal amount of actions was chosen to be 150. Any users having more than 150 actions are used for training/testing. In order to have both a training and test set for every user, the data is split up. 70\% is used for training and 30\% is used for the test set. This is done separately for every user, making sure that each user has the same 70--30 split. The network expects only real-valued numbers from the range [0,1], however because the features contain integer values, these values have to be normalized to fit into the range. This is done by taking the maximum value for every column and dividing every value in that column by that maximum, linearly scaling every value down to the range [0,1]. This is done by applying the formula below to every row, where \(x\) is the old row and \(x'\) is the new row:

$$ x' = x / \max (x) $$

This operation is performed for the training and test set at the same time before they are split up, ensuring that the scaling factor is the same for both sets. The data is kept in chronological order as it was read originally, ensuring that the input data closely resembles real input data and making maximum use of the LSTM's ability to make sense of sequences.

Keep in mind that in a real-time scenario, scaling can not be done by using the same factor for both the training and test set as the eventual \(\max \) value is unknown, leading to values that fall above the [0,1] range. This can however be solved by taking the maximum possible or reasonable value as a scaling factor for both test sets (for example no user will ever access more computers than are available on the network and no user will have more seconds between their last action than are in a human lifetime).

%cSpell:words srivastava
\section{Training}
After preprocessing, the training data is then used as input for the networks (s). For performance reasons, a single network is created, which is then used as the base for every other network. Instead of creating a new network for every user, new weights are created that are then applied to the base network. This network consist of 3 layers, with the first two being stateful LSTMs (stateful meaning the state is preserved across batches), and the third layer being a dense layer. All using an internal representation vector (and layer output) size of \(feature\_size \), which is \enquote{between} the input and output sizes of \(feature\_size \) (as suggested in~\cite{heaton2008introduction}). The network uses a batch size of 32. This number was chosen because increasing this value would decrease the weight of individual actions when in the testing stage as the network always only accepts input of that size and outputs a single \enquote{loss} value. Increasing the batch size would reduce the significance of a single anomaly on the loss value. Reducing the batch size however, quickly slows down the network by a lot. The network is trained first on the supplied training data, always trying to optimize for the lowest loss value using the mean squared error function, which measures the average of the squares of the deviations, giving an approximation of the deviation from the expected value. This is repeated for 25 epochs. This value was chosen because increasing this value would introduce overfitting and reducing it would result in higher loss values overall. As another measure to prevent overfitting, a dropout factor of 0.5, and a recurrent dropout factor of 0.2 is used for both LSTM layers. A dropout factor, which randomly drops certain input vectors, and a recurrent dropout factor that randomly drops out vectors between states, were shown to prevent overfitting in~\cite{srivastava2014dropout}. Note that these values are not perfect, seeing as there was no way to objectively compare the results of different parameters.

\section{Testing}\label{sec:methods:testing}
After training, the network is applied to the test set. There is a limitation requiring the use of the same batch size for both training and testing, which would downplay the significance of single anomalies (as in a set of \(batch\_size\) losses, one anomaly is not that significant), a method needs to be devised to test on a batch size of 1 instead. This is done by creating another network, identical in structure, and transferring the weights and states when tests occur. This has the same effect as changing the original network's batch size to 1 (for this application), but without all the performance losses.

The losses from all of the test data is then collected, after which the interquartile range (IQR) is calculated. The IQR function attempts to find statistical outliers based on the median values of a distribution. This is done by calculating the medians of both the upper and lower half of a distribution, who are then called Q1 and Q3 respectively. The IQR is then equal to \(Q3 - Q1\). Any values that lay outside of the ranges of the following functions, where \(x\) is the input value, are then called outliers.

$$ x < Q1 - 1.5 IQR $$

$$ x > Q3 + 1.5 IQR $$

In practice however, the first form of outlier will almost never be found, as that would mean an action so perfectly fits the user, it is an anomaly, which is very unlikely to point to actual anomalous behavior.

After finding anomalies, they have to be translated into actual rows. This issue occurs because when inputting solely features, the original input is discarded. As network administrators will want to see the name of the user that is behind a found anomaly and may want to investigate the actions themselves, these anomalies are translated back into source actions. This is done by storing the index of an anomaly and the user associated with the anomaly. The indexes have a 1--to--1 correspondence to the source actions, allowing for easy translation. This step can of course be skipped if for example, no anomalies were found, or only known anomalies were found.

%cSpell:words chollet
\section{Code}
All the code was written in Python, using the~\cite{chollet2015keras} Keras deep learning library's LSTM as the neural network, using all default settings except for those mentioned. Due to both the preprocessing/feature generation and the training/testing stages being very slow (as will be explained later in the evaluation section), especially when using big datasets, both of these operations can be fully parallelized. The first stage (preprocessing/feature generation) is a very CPU-dependent task, this task can be split over any amount of CPU's, handling a single user per CPU until all users have been processed. The second stage (training/testing) is a very GPU-dependent task. Because Keras requires a single GPU per process, multiple independent processes have to be launched in order to parallelize. This is done by having one root process splitting the to-do job between \(n\) processes, where \(n\) is the amount of processes. These \(n\) processes all produce partial outputs (both anomalies and plots), that have to then be stitched together by the host process. The host process then produces the final output.