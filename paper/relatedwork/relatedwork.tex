\chapter{Related Work}\label{ch:related_work}

%cSpell:words roesch portnoy intrusiondetection
The field of anomaly detection has recently been a very active field of research, becoming even more active as the amount and the complexity of the to data sets increases. In~\cite{roesch1999snort}, a lightweight system to scan a network's active data flow and to find possible intrusions based on known attacks was proposed. In~\cite{lee1998data}, simple classifiers were used to find anomalous behavior based on known intrusion patterns and changes in user behavior. More advanced techniques like clustering have also been used to find anomalies in unlabeled data sets. Since 2001 there were attempts to build a system that also detected yet unknown intrusion patterns and anomalous changes in user behavior using clustering, attempting to go beyond the constant search for new intrusion patterns that felt like a cat-and-mouse game for the developers of these systems, as explored by~\cite{Portnoy01intrusiondetection}.

%cSpell:words cannady lecun
With the upcoming field of deep learning in machine learning, the interest for applying these fields to anomaly and intrusion detection has also increased greatly. They were shown to have great potential, as explained in~\cite{lecun2015deep}. In~\cite{ryan1998intrusion}, a simple backpropagation neural network was applied to the terminal commands a user executed, an attempt was made to identify users by these commands in order to find any deviations, finding that this is an effective way of detecting intrusions. Contrary to rule-based analysis, neural networks perform a lot better on noisy data where some fields may be missing or incomplete, as~\cite{cannady1998artificial} shows. Here a neural network was applied to noisy computer network metadata in order to detect different methods of attack.

%cSpell:words bengio hochreiter
In a recurrent neural network, further explained in~\cite{lecun2015deep}, the output of one cell is connected to the input of the next cell. As a result processed information from previous cells' inputs should make it through to later cells. The information that is fed forward is selected based on what the network is trained to select. In theory, the RNN has the ability to recall previous inputs. In practice, however, standard RNNs seem to fall off when remembering for longer periods of time, often not remembering the data for more than about 5--6 iterations. This problem was investigated in~\cite{bengio1994learning} among others, finding problems with gradient based learning algorithms when applied to RNNs. This then prompted the development of the now commonly used Long Short Term Memory (LSTM) architecture for RNNs, which was introduced in~\cite{hochreiter1997long}.

%cSpell:words malhotra olsson akent
RNNs using the LSTM architecture proved very useful in finding so-called time series anomalies, which are anomalies over a time frame with multiple actions, instead of single-action anomalies. LSTM networks excel at this due to their ability to learn which aspects of the previous input data should be remembered and which aspects to forget. This is shown in~\cite{malhotra2015long}, where a stacked LSTM, trained to recognize regular behavior, was demonstrated to perform well on 4 different data sets. Each of these 4 data sets contains some anomalies, ranging from long-term to short-term anomalies, and from data containing a single variable to 12 variables. LSTM networks tend to be able to find so-called collective anomalies where other types of anomaly detection would not find them, being able to link together multiple instances of slightly deviating behavior into a definitive anomaly. This technique was applied in~\cite{olsson2015probabilistic}, where they were able to probabilistically group together the contribution of individual anomalies in order to find significant anomalous groups of cases.